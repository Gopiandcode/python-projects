{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a first graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "\n",
    "f = x * x * y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    \n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing multiple graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x1 = tf.Variable(1)\n",
    "    print(x1.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x2 = tf.Variable(2)\n",
    "    print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle of node values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # x is recomputed for each call - not efficient\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # x is reused for each evaluation - efficient\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 9), (20640, 9))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias.shape, scaled_housing_data_plus_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "# theta = (Xt X)' XT y\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.matmul(X, theta)\n",
    "rmse = tf.sqrt(tf.math.reduce_sum(tf.math.square(tf.math.subtract(y_pred, y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [[-3.63148041e+01]\n",
      " [ 4.38327283e-01]\n",
      " [ 9.54509154e-03]\n",
      " [-1.09111905e-01]\n",
      " [ 6.51177227e-01]\n",
      " [-3.67034363e-06]\n",
      " [-3.80339054e-03]\n",
      " [-4.14844275e-01]\n",
      " [-4.27279472e-01]]\n",
      "rmse :  104.0312\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_val, rmse_val = sess.run([theta, rmse])\n",
    "    print('theta: ', theta_val)\n",
    "    print('rmse : ', rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent Linear Regression TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  10.159446\n",
      "Epoch 100 MSE =  9.908106\n",
      "Epoch 200 MSE =  9.669728\n",
      "Epoch 300 MSE =  9.443579\n",
      "Epoch 400 MSE =  9.228973\n",
      "Epoch 500 MSE =  9.025283\n",
      "Epoch 600 MSE =  8.831894\n",
      "Epoch 700 MSE =  8.648241\n",
      "Epoch 800 MSE =  8.473796\n",
      "Epoch 900 MSE =  8.308061\n",
      "[[ 0.49345446]\n",
      " [-0.4639052 ]\n",
      " [ 0.5981647 ]\n",
      " [-0.549965  ]\n",
      " [ 0.66150874]\n",
      " [-0.04208593]\n",
      " [ 0.6210185 ]\n",
      " [ 0.32942343]\n",
      " [-0.01300348]] [[-5.099454  ]\n",
      " [-5.11695   ]\n",
      " [-3.5277438 ]\n",
      " ...\n",
      " [-0.2962908 ]\n",
      " [-0.17037159]\n",
      " [-0.440858  ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "# gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    best_error = error.eval()\n",
    "    print(best_theta,best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  10.585614\n",
      "Epoch 100 MSE =  4.9146767\n",
      "Epoch 200 MSE =  4.8499827\n",
      "Epoch 300 MSE =  4.8375635\n",
      "Epoch 400 MSE =  4.828945\n",
      "Epoch 500 MSE =  4.822579\n",
      "Epoch 600 MSE =  4.817858\n",
      "Epoch 700 MSE =  4.814344\n",
      "Epoch 800 MSE =  4.8117156\n",
      "Epoch 900 MSE =  4.8097425\n",
      "[[ 0.20732474]\n",
      " [ 0.8546698 ]\n",
      " [ 0.14351124]\n",
      " [-0.27533066]\n",
      " [ 0.29730058]\n",
      " [ 0.00417901]\n",
      " [-0.04195486]\n",
      " [-0.69587284]\n",
      " [-0.6676284 ]] [[-2.4477556]\n",
      " [-1.6824293]\n",
      " [-1.8989267]\n",
      " ...\n",
      " [-2.678736 ]\n",
      " [-2.4642355]\n",
      " [-2.30393  ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimiser.minimize(mse)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    best_error = error.eval()\n",
    "    print(best_theta,best_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6., 7., 8.]], dtype=float32), (1, 3), array([[ 9., 10., 11.],\n",
       "        [12., 13., 14.]], dtype=float32), (2, 3))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6], [7,8,9]]})\n",
    "\n",
    "B_val_1, B_val_1.shape, B_val_2, B_val_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias[0:100,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model. Training from scratch\n",
      "Epoch 0 MSE =  [25858.63]\n",
      "Epoch 100 MSE =  [28317.373]\n",
      "Epoch 200 MSE =  [28317.373]\n",
      "Epoch 300 MSE =  [28317.373]\n",
      "Epoch 400 MSE =  [28317.373]\n",
      "Epoch 500 MSE =  [28317.373]\n",
      "Epoch 600 MSE =  [28317.373]\n",
      "Epoch 700 MSE =  [28317.373]\n",
      "Epoch 800 MSE =  [28317.373]\n",
      "Epoch 900 MSE =  [28317.373]\n",
      "[[ 8.1041384e-01]\n",
      " [ 9.9553329e-01]\n",
      " [ 9.7243279e-02]\n",
      " [-3.6800289e-01]\n",
      " [ 3.5647058e-01]\n",
      " [-3.5688521e-03]\n",
      " [ 9.6779538e-04]\n",
      " [-2.3816462e-01]\n",
      " [-1.4885532e+00]] [[178.13167]\n",
      " [169.92992]\n",
      " [178.08241]\n",
      " ...\n",
      " [169.17659]\n",
      " [170.58711]\n",
      " [168.47897]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimiser.minimize(mse)\n",
    "\n",
    "def fetch_batch(epoch, i, batch_size):\n",
    "    X_batch = scaled_housing_data_plus_bias[i * batch_size: min(m, (i+1) * batch_size), :]\n",
    "    y_batch = housing.target.reshape(-1,1)[i * batch_size: min(m, (i+1) * batch_size), :]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "saving = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if saving:\n",
    "        try:\n",
    "            saver.restore(sess, '/tmp/model.ckpt')\n",
    "        except ValueError:\n",
    "            print('no saved model. Training from scratch')\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(\n",
    "                training_op, \n",
    "                 feed_dict={ X: X_batch, y: y_batch }\n",
    "            )\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            mse_val = sess.run([mse], feed_dict={\n",
    "                X: housing_data_plus_bias,\n",
    "                y: housing.target.reshape(-1,1)\n",
    "            })\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse_val)\n",
    "            if saving:\n",
    "                saver.save(sess, '/tmp/model.ckpt')\n",
    "        \n",
    "    best_theta, best_error = sess.run(\n",
    "        [theta, error],\n",
    "        feed_dict={\n",
    "                X: housing_data_plus_bias,\n",
    "                y: housing.target.reshape(-1,1)\n",
    "            }\n",
    "    )\n",
    "    print(best_theta,best_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the graph and training curves using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190205113346'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_minibatch_gradient_descent(data, labels, graph=None, learning_rate=0.01, batch_size=100, n_epochs=1000, saving=False, logging=False, log_dir=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Performs minibatch gradient descent to train an OLS model over the provided data\n",
    "    \n",
    "    Parameters\n",
    "    ~~~~~~~~~~~\n",
    "    \n",
    "    :param data: the training data \n",
    "    \n",
    "    :param labels: the training labels\n",
    "    \n",
    "    :param graph: if provided, the model will be placed into the provided graph.\n",
    "    \n",
    "    :param learning_rate: the learning rate with which to train the mse optimiser\n",
    "    \n",
    "    :param batch_size: the number of batches to use in minibatch gradient descent.\n",
    "    \n",
    "    :param n_epochs: the number of epochs with which to train.\n",
    "    \n",
    "    :param saving: whether the best model seen should be saved, if true, then the save_dir must also be provided\n",
    "    \n",
    "    :param save_dir: directory to save models to\n",
    "    \n",
    "    :param logging: whether the training progress should be logged. If true then the log_dir must also be provided\n",
    "    \n",
    "    \"\"\"\n",
    "    m, n = data.shape\n",
    "    \n",
    "    # parameter checking\n",
    "    assert labels.shape[0] == m, \"Invalid labels shape\"\n",
    "    assert labels.shape[1] == 1, \"OLS requires a single continuous output label\"\n",
    "    assert not logging or log_dir\n",
    "    assert not saving or save_dir       \n",
    "    \n",
    "    \n",
    "    # if no graph is provided, just create a new one\n",
    "    if not graph:\n",
    "        graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "    \n",
    "        # calculate the number of batches\n",
    "        n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "        # set up placeholders for inputs\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n), name='X')\n",
    "        y = tf.placeholder(tf.float32, shape=(None,1), name='y')\n",
    "\n",
    "        # model parameters to be learned\n",
    "        theta = tf.Variable(tf.random_uniform([n,1], -1.0, 1.0), name='theta')\n",
    "\n",
    "        # predictions\n",
    "        y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "\n",
    "        with tf.name_scope('loss') as scope:\n",
    "            # error function and optimiser for learning\n",
    "            error = y_pred - y\n",
    "            mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "            \n",
    "        optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimiser.minimize(mse)\n",
    "\n",
    "\n",
    "        # additional configurations before training\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        if logging:\n",
    "            log_dir = '{}/run-{}/'.format(log_dir, now)\n",
    "\n",
    "\n",
    "        saver = None \n",
    "        file_writer = None\n",
    "        mse_summary = tf.summary.scalar('MSE', mse)\n",
    "\n",
    "        if saving:\n",
    "            print('INFO: Saving Enabled. Saving models to {}'.format(save_dir))\n",
    "            saver = tf.train.Saver()\n",
    "        if logging:\n",
    "            print('INFO: Logging Enabled. Logging training to {}'.format(log_dir))\n",
    "            file_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        last_mse = None\n",
    "\n",
    "\n",
    "        def fetch_next_batch(batch_index):\n",
    "            lower_bound = batch_index * batch_size\n",
    "            upper_bound = min((batch_index + 1) * batch_size, m)\n",
    "            return data[lower_bound:upper_bound, :], labels[lower_bound:upper_bound, :]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # initialise all variables\n",
    "            if saving:\n",
    "                # try loading prior model\n",
    "                try:\n",
    "                    saver.restore(sess, save_dir)\n",
    "                except ValueError:\n",
    "                    print('INFO: Could not load previously saved model - must train from scratch.')\n",
    "                    sess.run(init)\n",
    "            else:\n",
    "                sess.run(init)\n",
    "\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                for batch_index in range(n_batches):\n",
    "                    X_batch, y_batch = fetch_next_batch(batch_index)\n",
    "                    \n",
    "                    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "                    if logging and batch_index % 10 == 0:\n",
    "                        summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        step = epoch * n_batches + batch_index\n",
    "                        file_writer.add_summary(summary_str, step)\n",
    "\n",
    "                current_mse = sess.run(mse, feed_dict={X: data, y: labels})\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print('EPOCH', epoch, ', MSE = ', current_mse)\n",
    "\n",
    "                    if saving and (last_mse is None or last_mse > current_mse):\n",
    "                        print('INFO: Saving model to save dir - old: {} -> new: {}'.format(last_mse, current_mse))\n",
    "                        last_mse = current_mse\n",
    "                        saver.save(sess, save_dir)\n",
    "\n",
    "            # having trained, evaluate the model again\n",
    "            current_mse = sess.run(mse, feed_dict={X: data, y: labels})\n",
    "            if saving and (last_mse is None or last_mse > current_mse):\n",
    "                print('INFO: Saving final model to save dir - new: {}'.format(current_mse))\n",
    "                last_mse = current_mse\n",
    "                saver.save(sess, save_dir)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saving Enabled. Saving models to learning_models/saves/saved_model\n",
      "INFO: Logging Enabled. Logging training to learning_models/logs/run-20190205124812/\n",
      "INFO:tensorflow:Restoring parameters from learning_models/saves/saved_model\n",
      "EPOCH 0 , MSE =  4.9263\n",
      "INFO: Saving model to save dir - old: None -> new: 4.926300048828125\n",
      "EPOCH 100 , MSE =  4.9263\n",
      "EPOCH 200 , MSE =  4.9263\n",
      "EPOCH 300 , MSE =  4.9263\n",
      "EPOCH 400 , MSE =  4.9263\n",
      "EPOCH 500 , MSE =  4.9263\n",
      "EPOCH 600 , MSE =  4.9263\n",
      "EPOCH 700 , MSE =  4.9263\n",
      "EPOCH 800 , MSE =  4.9263\n",
      "EPOCH 900 , MSE =  4.9263\n"
     ]
    }
   ],
   "source": [
    "perform_minibatch_gradient_descent(scaled_housing_data_plus_bias, np.reshape(housing.target, (-1,1)), saving=True, save_dir='learning_models/saves/saved_model', logging=True, log_dir='learning_models/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function perform_minibatch_gradient_descent in module __main__:\n",
      "\n",
      "perform_minibatch_gradient_descent(data, labels, graph=None, learning_rate=0.01, batch_size=100, n_epochs=1000, saving=False, logging=False, log_dir=None, save_dir=None)\n",
      "    Performs minibatch gradient descent to train an OLS model over the provided data\n",
      "    \n",
      "    Parameters\n",
      "    ~~~~~~~~~~~\n",
      "    \n",
      "    :param data: the training data \n",
      "    \n",
      "    :param labels: the training labels\n",
      "    \n",
      "    :param graph: if provided, the model will be placed into the provided graph.\n",
      "    \n",
      "    :param learning_rate: the learning rate with which to train the mse optimiser\n",
      "    \n",
      "    :param batch_size: the number of batches to use in minibatch gradient descent.\n",
      "    \n",
      "    :param n_epochs: the number of epochs with which to train.\n",
      "    \n",
      "    :param saving: whether the best model seen should be saved, if true, then the save_dir must also be provided\n",
      "    \n",
      "    :param save_dir: directory to save models to\n",
      "    \n",
      "    :param logging: whether the training progress should be logged. If true then the log_dir must also be provided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(perform_minibatch_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
